{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b1c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils import preprocess_function, encode_rare_chars\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adbaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/wikisql'\n",
    "dataset = load_dataset(path+'/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07131d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = dataset.map(preprocess_function, batched=True, batch_size=2048)\n",
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocessed_dataset[\"train\"]\n",
    "val_data = preprocessed_dataset[\"validation\"]\n",
    "test_data = preprocessed_dataset[\"test\"]\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c98721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine inputs and labels into a single list of text data\n",
    "text_data = []\n",
    "for sample in tqdm(preprocessed_dataset[\"train\"]):\n",
    "    text_data.append(sample['input_text'])\n",
    "    text_data.append(sample['label_text'])\n",
    "\n",
    "# Save text data to a plain text file\n",
    "output_file = \"tokenizer_training_data2.txt\"\n",
    "sep_count = 0\n",
    "newline_count = 0\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in text_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "        newline_count = newline_count + 1\n",
    "        sep_count = sep_count + line.count('[SEP]')\n",
    "\n",
    "print(sep_count, newline_count)\n",
    "print(f\"Data written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "file_path = 'tokenizer_training_data2.txt'\n",
    "\n",
    "# Step 1: Read the content of the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Count occurrences of each character\n",
    "char_counts = Counter(text)\n",
    "\n",
    "# Correct for inserted newlines\n",
    "print(char_counts[\"\\n\"])\n",
    "char_counts[\"\\n\"] -= newline_count\n",
    "print(char_counts[\"\\n\"])\n",
    "\n",
    "# Step 3: Separate characters and their frequencies\n",
    "characters = list(char_counts.keys())\n",
    "frequencies = list(char_counts.values())\n",
    "\n",
    "# Step 4: Get the most common characters\n",
    "most_common_chars = char_counts.most_common()\n",
    "\n",
    "# Step 5: Calculate cumulative coverage\n",
    "total_characters = sum(char_counts.values())\n",
    "cumulative_frequencies = []\n",
    "cumulative_sum = 0\n",
    "\n",
    "for _, freq in most_common_chars:\n",
    "    cumulative_sum += freq\n",
    "    cumulative_frequencies.append((cumulative_sum / total_characters) * 100)\n",
    "\n",
    "# Step 6: Define a character coverage threshold\n",
    "coverage_threshold = 99.85  # Adjust this value as needed\n",
    "\n",
    "# Find the index of the last character needed to fulfill the coverage\n",
    "coverage_index = next(i for i, freq in enumerate(cumulative_frequencies) if freq >= coverage_threshold)\n",
    "\n",
    "# Get the set of characters within the coverage\n",
    "covered_chars = set(char for char, _ in most_common_chars[:coverage_index + 1])\n",
    "\n",
    "print(covered_chars)\n",
    "print(len(covered_chars))\n",
    "\n",
    "# Step 7: Create a mapping for all characters (covered map to themselves, uncovered to special sequence)\n",
    "out_of_coverage_chars = set(characters) - covered_chars\n",
    "mapping = {char: f\"[MAP]{i}[/MAP]\" for i, char in enumerate(out_of_coverage_chars)}\n",
    "\n",
    "# Step 8: Save the mapping and reverse mapping\n",
    "reverse_mapping = {v: k for k, v in mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mapping_file_path = 'mapping.json'\n",
    "reverse_mapping_file_path = 'reverse_mapping.json'\n",
    "\n",
    "with open(mapping_file_path, 'w', encoding='utf-8') as mapping_file:\n",
    "    json.dump(mapping, mapping_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(reverse_mapping_file_path, 'w', encoding='utf-8') as reverse_mapping_file:\n",
    "    json.dump(reverse_mapping, reverse_mapping_file, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2c04f07541c2b06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba68984-db84-4e18-b82d-640dbeff4452",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_encoded_dataset = preprocessed_dataset.map(lambda batch: encode_rare_chars(batch, mapping), batched=True, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3755a77-4487-4c96-9cb6-1b13b5460120",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset[\"train\"][\"label_text\"][46344]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preprocessed_encoded_dataset[\"train\"][\"label_text\"][46344]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3230351-dd36-406b-be2c-b230924928ec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8c162-8274-4779-a74d-fcefe7f5d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine inputs and labels into a single list of text data\n",
    "text_data = []\n",
    "for sample in tqdm(preprocessed_encoded_dataset[\"train\"]):\n",
    "    text_data.append(sample['input_text'])\n",
    "    text_data.append(sample['label_text'])\n",
    "\n",
    "# Save text data to a plain text file\n",
    "output_file = \"encoded_tokenizer_training_data.txt\"\n",
    "sep_count = 0\n",
    "map_count = 0\n",
    "newline_count = 0\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in text_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "        newline_count = newline_count + 1\n",
    "        sep_count = sep_count + line.count('[SEP]')\n",
    "        map_count = map_count + line.count('[MAP]')\n",
    "\n",
    "print(sep_count, map_count, newline_count)\n",
    "print(f\"Data written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe40280-c67d-4350-9045-454233a3b09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
