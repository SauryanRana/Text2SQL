{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:34:51.199453800Z",
     "start_time": "2024-11-07T19:34:51.197472300Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer, PreTrainedTokenizerFast, convert_slow_tokenizer\n",
    "from datasets import load_dataset\n",
    "from utils import preprocess_function, parse_sql_to_canonical, tokenize\n",
    "import torch\n",
    "import json\n",
    "from lib.dbengine import DBEngine\n",
    "from lib.query import Query\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b21fc68cd6c30d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:35:00.331141100Z",
     "start_time": "2024-11-07T19:34:59.527861Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Path to checkpoint folder\n",
    "checkpoint_path = \"../models/2_heads_2e-4_lr_constant_512MappingTokenizer_128_bs_32_dff_1\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2005d23-50e7-4cbb-b247-c0e6b5c58049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=convert_slow_tokenizer.convert_slow_tokenizer(T5Tokenizer(\"tokenizers/sp_512_bpe_encoded.model\", legacy=False, load_from_cache_file=False)))\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72785623-0baa-4732-b94f-3de6bec77c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/wikisql'\n",
    "dataset = load_dataset(path+'/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e1534411-afc0-403a-8f89-dc0972d192d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['phase', 'question', 'table', 'sql'],\n",
      "    num_rows: 1\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "index = 1338  # TODO find smart way to choose random samples\n",
    "sample = dataset[\"test\"].select(range(index, index+1))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5dc03c7b-e36b-44ad-ad07-cf216643ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# alternatively overwrite question here if people want to try custom questions on the same table\n",
    "# display table\n",
    "# sample.question[0] = \"Custom question about the same table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fb9e12f2-b917-40da-8e4f-74cc7e4bf69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aceb50920394d10bd578d10edf66741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What theme name has the original artist of Dolly Parton?[SEP]Week #[SEP]Theme[SEP]Song choice[SEP]Original artist[SEP]Order #[SEP]Result']\n",
      "['SELECT Theme FROM table WHERE Original artist = Dolly Parton']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_sample = sample.map(preprocess_function, batched=True) # concatenates questions with headers using custom [SEP] token\n",
    "print(preprocessed_sample[\"input_text\"])\n",
    "print(preprocessed_sample[\"label_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e76b211e-0830-42f1-9f54-931f0b2e5f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95b21e729e74302889d30559123de6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode rare tokens because the tokenizer doesn't know them\n",
    "mapping_file_path = 'mapping.json'\n",
    "reverse_mapping_file_path = 'reverse_mapping.json'\n",
    "\n",
    "with open(mapping_file_path, 'r', encoding='utf-8') as mapping_file:\n",
    "    mapping = json.load(mapping_file)\n",
    "\n",
    "with open(reverse_mapping_file_path, 'r', encoding='utf-8') as reverse_mapping_file:\n",
    "    reverse_mapping = json.load(reverse_mapping_file)\n",
    "\n",
    "encoded_preprocessed_sample = preprocessed_sample.map(lambda sample: encode_rare_chars(sample, mapping), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "60c5c36a-8f23-4501-b540-79c7286aa478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12e11be10e647ac8acda8832bb3e6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[64, 21, 345, 333, 103, 74, 168, 21, 332, 14, 296, 332, 174, 252, 51, 129, 94, 341, 356, 68, 174, 10, 372, 3, 349, 246, 270, 3, 353, 16, 345, 333, 3, 351, 10, 355, 69, 342, 336, 55, 333, 3, 361, 339, 296, 332, 174, 252, 3, 361, 339, 346, 8, 270, 3, 248, 2, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511]]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sample = encoded_preprocessed_sample.map(lambda sample: tokenize(sample, tokenizer), batched=True)\n",
    "print(tokenized_sample[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7657feba1bd54b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:35:02.922098Z",
     "start_time": "2024-11-07T19:35:02.919096500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def inference(input_ids) -> str:\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        outputs = model.generate(input_ids=torch.tensor(input_ids), num_beams=10, max_length=128)\n",
    "    output = tokenizer.decode(token_ids=outputs[0][1:], skip_special_tokens=True) # for some reason the beginning of sentence token doesn't get removed properly so we cut it off manually\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ba6b9730-f9e0-4d15-a679-0615ba11c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(query, table_header, reverse_mapping):\n",
    "    cleaned_canonical = parse_sql_to_canonical(query, table_header, reverse_mapping)\n",
    "    cleaned_query = None\n",
    "    # cleaned_query = make_canonical_human_readable  TODO (we need this function; remove line above after wards)\n",
    "    return cleaned_canonical, cleaned_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f1c618cf85c52fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:35:36.281157900Z",
     "start_time": "2024-11-07T19:35:34.837235100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: None\n",
      "Correct query: SELECT Theme FROM table WHERE Original artist = Dolly Parton\n"
     ]
    }
   ],
   "source": [
    "pred_canonical, pred_human_readable = post_processing(inference(tokenized_sample[\"input_ids\"]), sample[\"table\"][0][\"header\"], reverse_mapping)\n",
    "correct_canonical, correct_human_readable = post_processing(sample[\"sql\"][0][\"human_readable\"], sample[\"table\"][0][\"header\"], reverse_mapping) # I know this line is stupid but I don't have a better way to get the proper canonical form for the solutions. The one in the data is in a weird format not supported by the db_engine\n",
    "print(f\"Model output: {pred_human_readable}\")\n",
    "print(f\"Correct query: {sample[\"sql\"][0][\"human_readable\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8e82f564-f302-4bc9-a140-55de992e29b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: {'sel': 1, 'agg': 0, 'conds': {(3, 0, 'Dolly Parton')}}\n",
      "Correct query: {'sel': 1, 'agg': 0, 'conds': {(3, 0, 'Dolly Parton')}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model output: {pred_canonical}\")\n",
    "print(f\"Correct query: {correct_canonical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e28c5437-8a0b-4ec7-a975-b9f70d650e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query result: ['dolly parton']\n",
      "Correct result: ['dolly parton']\n"
     ]
    }
   ],
   "source": [
    "db_path = os.path.abspath(path+'/tables/test/test.db')\n",
    "if not os.path.exists(db_path):\n",
    "    raise FileNotFoundError(f\"Database file not found: {db_path}\")\n",
    "db_engine = DBEngine(db_path)\n",
    "\n",
    "table_id = sample[\"table\"][0][\"id\"]\n",
    "pred_query = Query.from_dict(pred_canonical)\n",
    "correct_query = Query.from_dict(correct_canonical)\n",
    "try:\n",
    "    pred_result = db_engine.execute_query(table_id, pred_query)\n",
    "except Exception as e:\n",
    "    pred_result = f\"Execution error: {e}\"\n",
    "\n",
    "try:\n",
    "    gold_result = db_engine.execute_query(table_id, correct_query)\n",
    "except Exception as e:\n",
    "    gold_result = f\"Execution error: {e}\"\n",
    "\n",
    "print(f\"Query result: {pred_result}\")\n",
    "print(f\"Correct result: {gold_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286b78d-6890-44ce-8d48-11ccc71390b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07cd5a5-6ca4-486f-9306-9ab87699e260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
