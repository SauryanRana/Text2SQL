{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from utils import preprocess_function, tokenize, filter_function\n",
    "from transformers import T5Tokenizer, PreTrainedTokenizerFast, convert_slow_tokenizer\n",
    "from lib.dbengine import DBEngine\n",
    "from lib.query import Query\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../datasets/wikisql/data'\n",
    "dataset = load_dataset(path)\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450466d26bc4d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/t5-efficient-tiny\")\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=convert_slow_tokenizer.convert_slow_tokenizer(T5Tokenizer(\"tokenizers/sp_2k_bpe_1.model\", legacy=False)))\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "preprocessed_dataset = dataset.map(preprocess_function, batched=True, batch_size=2048)\n",
    "tokenized_dataset = preprocessed_dataset.map(lambda batch: tokenize(batch, tokenizer, input_max_length=None, output_max_length=None, padding=\"do_not_pad\"), batched=True, batch_size=2048, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a34d339f8f1afd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_train_data = tokenized_dataset[\"train\"]\n",
    "tokenized_val_data = tokenized_dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac88943afcace7d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sequence_length_histograms(data):\n",
    "    # Calculate sequence lengths for inputs and labels\n",
    "    input_lengths = [len(seq) for seq in data[\"input_ids\"]]\n",
    "    label_lengths = [len(seq) for seq in data[\"labels\"]]\n",
    "    \n",
    "    # Determine bins\n",
    "    bins_inputs = max(input_lengths) - min(input_lengths) + 1\n",
    "    bins_labels = max(label_lengths) - min(label_lengths) + 1\n",
    "    \n",
    "    # Create side-by-side histograms\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "    \n",
    "    # Plot histogram for input lengths\n",
    "    axes[0].hist(input_lengths, bins=bins_inputs, alpha=0.7, color='blue', edgecolor='black')\n",
    "    axes[0].axvline(np.mean(input_lengths), color='red', linestyle='dashed', linewidth=1, label='Mean Length')\n",
    "    axes[0].set_xlabel('Input Sequence Length')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Histogram of Input Sequence Lengths')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot histogram for label lengths\n",
    "    axes[1].hist(label_lengths, bins=bins_labels, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1].axvline(np.mean(label_lengths), color='red', linestyle='dashed', linewidth=1, label='Mean Length')\n",
    "    axes[1].set_xlabel('Label Sequence Length')\n",
    "    axes[1].set_title('Histogram of Label Sequence Lengths')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d275c2121161044",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_length_histograms(tokenized_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53a67b-8080-4b12-b822-a3e7d8b88705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'tokenizer_training_data.txt'\n",
    "\n",
    "# Step 1: Read the content of the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Count occurrences of each character\n",
    "char_counts = Counter(text)\n",
    "\n",
    "# Step 3: Separate characters and their frequencies\n",
    "characters = list(char_counts.keys())\n",
    "frequencies = list(char_counts.values())\n",
    "\n",
    "# Step 4: Plot the full histogram without labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(characters)), frequencies, color='skyblue')\n",
    "plt.title('Character Frequency Histogram (Full)')\n",
    "plt.xlabel('Character Index')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()  # Adjust layout to fit\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Get the most common characters\n",
    "most_common_count = 80  # Adjust this number as needed\n",
    "most_common_chars = char_counts.most_common(most_common_count)\n",
    "\n",
    "# Step 6: Calculate cumulative coverage\n",
    "total_characters = sum(char_counts.values())\n",
    "cumulative_frequencies = []\n",
    "cumulative_sum = 0\n",
    "\n",
    "for _, freq in most_common_chars:\n",
    "    cumulative_sum += freq\n",
    "    cumulative_frequencies.append((cumulative_sum / total_characters) * 100)\n",
    "\n",
    "# Step 7: Create a table for the most common characters\n",
    "table_data = {\n",
    "    \"Character\": [repr(c[0]) for c in most_common_chars],\n",
    "    \"Frequency\": [c[1] for c in most_common_chars],\n",
    "    \"Coverage (%)\": [round(cov, 2) for cov in cumulative_frequencies],\n",
    "}\n",
    "table = pd.DataFrame(table_data)\n",
    "\n",
    "# Print the table\n",
    "print(f\"Top {most_common_count} Most Common Characters and Cumulative Coverage:\")\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad2590ae5e15774",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_lengths = [len(seq) for seq in tokenized_train_data[\"input_ids\"]]\n",
    "sorted_input_indices = sorted(range(len(input_lengths)), key=lambda i: input_lengths[i])\n",
    "label_lengths = [len(seq) for seq in tokenized_train_data[\"labels\"]]\n",
    "sorted_label_indices = sorted(range(len(label_lengths)), key=lambda i: label_lengths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d437cbd7a92c256",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_train_data[sorted_label_indices[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b34b0fd6da6aee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_train_data = tokenized_train_data.filter(lambda sample: filter_function(sample, tokenizer))\n",
    "filtered_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04974e51c0236d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_length_histograms(filtered_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68445b8e3fc782f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_path = '../datasets/wikisql/tables/train/train.db'\n",
    "db_engine = DBEngine(db_path)\n",
    "\n",
    "def empty_response_filter(batch):\n",
    "    keep = []\n",
    "    for table, sql in zip(batch[\"table\"], batch[\"sql\"]):\n",
    "        sql[\"conds\"] = list(zip(sql[\"conds\"][\"column_index\"], sql[\"conds\"][\"operator_index\"], sql[\"conds\"][\"condition\"]))\n",
    "        query = Query.from_dict(sql)\n",
    "        gold_result = db_engine.execute_query(table[\"id\"], query)\n",
    "        if gold_result == [None]:\n",
    "            keep.append(1)\n",
    "        else:\n",
    "            keep.append(0)\n",
    "\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea5fd202ecf04c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_data = train_data.filter(empty_response_filter, batched=True)\n",
    "wrong_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc30d36f15464db",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "db_path = '../datasets/wikisql/tables/train/train.db'\n",
    "db_engine = DBEngine(db_path)\n",
    "\n",
    "def fix_commas(batch):\n",
    "    replaced = 0\n",
    "    total = 0\n",
    "    for batch_idx, (sql, table) in enumerate(zip(batch[\"sql\"], batch[\"table\"])):\n",
    "        sql_copy = deepcopy(sql)\n",
    "        sql_copy[\"conds\"] = list(zip(sql[\"conds\"][\"column_index\"], sql[\"conds\"][\"operator_index\"], sql[\"conds\"][\"condition\"]))\n",
    "        query = Query.from_dict(sql_copy)\n",
    "        gold_result = db_engine.execute_query(table[\"id\"], query)\n",
    "        if gold_result == [None]:\n",
    "            for cond_idx, (column_idx, condition) in enumerate(zip(sql[\"conds\"][\"column_index\"], sql[\"conds\"][\"condition\"])):\n",
    "                total = total + 1\n",
    "                if table[\"types\"][column_idx] == 'text':\n",
    "                    fixed_cond, count = re.subn(r'(?<!\\s)(?:(?<!\\d),|,(?!\\d))', ' ,', condition)\n",
    "                    batch[\"sql\"][batch_idx][\"conds\"][\"condition\"][cond_idx] = fixed_cond\n",
    "                    replaced = replaced + count\n",
    "    print(f'Replaced {replaced} out of {total} conditions.')\n",
    "    return batch\n",
    "\n",
    "maybe_fixed_data = train_data.map(fix_commas, batched=True, batch_size=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c27e012d81addf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_commas(batch):\n",
    "    replaced = 0\n",
    "    total = 0\n",
    "    for batch_idx, (sql, table) in enumerate(zip(batch[\"sql\"], batch[\"table\"])):\n",
    "        for cond_idx, (column, condition) in enumerate(zip(sql[\"conds\"][\"column_index\"], sql[\"conds\"][\"condition\"])):\n",
    "            total = total + 1\n",
    "            if table[\"types\"][column] == 'text':\n",
    "                batch[\"sql\"][batch_idx][\"conds\"][\"condition\"][cond_idx], count = re.subn(r'(?<!\\s)(?:(?<!\\d),|,(?!\\d))', ' ,', condition)\n",
    "                replaced = replaced + count\n",
    "    print(f'Replaced {replaced} out of {total} conditions.')\n",
    "    return batch\n",
    "\n",
    "maybe_fixed_data = train_data.map(fix_commas, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62db16150b4267a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_fixed_data = maybe_fixed_data.filter(empty_response_filter, batched=True)\n",
    "wrong_fixed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67adb217d3b863",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"ID\": [table[\"id\"] for table in wrong_fixed_data[\"table\"]],\n",
    "    \"Header\": [table[\"header\"] for table in wrong_fixed_data[\"table\"]],\n",
    "    \"Question\": wrong_fixed_data[\"question\"],\n",
    "    \"SQL Statement\": [sql[\"human_readable\"] for sql in wrong_fixed_data[\"sql\"]]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('wrong_fixed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d848a032fc6bb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Flatten the dataset to count unique tokens\n",
    "all_tokens = [token for seq in tokenized_train_data[\"input_ids\"] for token in seq]\n",
    "unique_tokens = set(all_tokens)\n",
    "print(f\"Number of unique tokens: {len(unique_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1bfdaf2e6ba34b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Token frequency distribution\n",
    "print(\"Calculating token frequencies...\")\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "# Plot histogram of token counts\n",
    "print(\"Plotting token frequency histogram...\")\n",
    "frequencies = list(token_counts.values())\n",
    "plt.hist(frequencies, bins=32, alpha=0.7, color='green', edgecolor='black', log=True)\n",
    "plt.xlabel('Token Frequency')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.title('Histogram of Token Frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c17af53ed57821",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 20 most common tokens\n",
    "print(\"Top 10 most common tokens:\")\n",
    "top_10_tokens = token_counts.most_common(20)\n",
    "print(f\"{'Token':<15}{'Count':<10}\")\n",
    "print(\"-\" * 25)\n",
    "for token, count in top_10_tokens:\n",
    "    print(f\"{str(tokenizer.decode(token)):<15}{count:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cf120648afcde",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
